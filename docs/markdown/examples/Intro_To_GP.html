

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>An Introduction to Gaussian Processes Models &mdash; Squidward  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Fitting A Gaussian Process" href="Fitting_A_GP.html" />
    <link rel="prev" title="Gaussian Process Classification" href="Classification.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Squidward
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Squidward Overview:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Squidward</a></li>
</ul>
<p class="caption"><span class="caption-text">Squidward Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Working_With_Kernels.html">Working with Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="Regression.html">Gaussian Process Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Heteroscedastic_GP.html">Heteroscedastic Gaussian Process Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="NonZero_Prior_Mean.html">Non-Zero Prior Mean for Gaussian Process Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="Classification.html">Gaussian Process Classification</a></li>
</ul>
<p class="caption"><span class="caption-text">Gaussian Process Tutorials:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">An Introduction to Gaussian Processes Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#who-is-this-notebook-intended-for">Who Is This Notebook Intended For?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-a-gaussian-process">What Is A Gaussian Process?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-a-gaussian-process-model">What is a Gaussian Process Model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-the-deal-with-kernels">What Is The Deal With Kernels?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advantages-of-gaussian-process-models">Advantages Of Gaussian Process Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#disadvantages-of-gaussian-process-models">Disadvantages Of Gaussian Process Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-have-we-learned">What Have We Learned?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#where-to-learn-more">Where To Learn More</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Fitting_A_GP.html">Fitting A Gaussian Process</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Squidward</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>An Introduction to Gaussian Processes Models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/markdown/examples/Intro_to_GP.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="an-introduction-to-gaussian-processes-models">
<h1>An Introduction to Gaussian Processes Models<a class="headerlink" href="#an-introduction-to-gaussian-processes-models" title="Permalink to this headline">¶</a></h1>
<div class="section" id="who-is-this-notebook-intended-for">
<h2>Who Is This Notebook Intended For?<a class="headerlink" href="#who-is-this-notebook-intended-for" title="Permalink to this headline">¶</a></h2>
<p>Many machine learning engineers have never even heard of a Gaussian process model. This is a real shame as they are quite interesting models that can be very useful in a variety of situations from Digital Marketing, Oil Drilling, Robotics, Aerospace Design, and Optimization. Gaussian processes are a relatively old methodology being used as far back as 1880 by astronomer T. N. Thiele and gaining populatrity in the 1970’s in geostatistics under the name <a class="reference external" href="https://en.wikipedia.org/wiki/Kriging">kriging</a> (pronaunced “kree-ging”).</p>
<p>This notebook is meant to be a quick and dirty introduction to Gaussian process models for those unfamiliar with the topic. I assume some basic familiarity with machine learning and Bayesian statistics. We won’t too deep into the math or the nitty gritty, but at the end of this crash course I provide links to much more in-depth and rigorous sources of information for those readers wishing to dive deeper.</p>
<p>This notebook is simply meant to whet the appetite of those interested in using Gaussian process models. I’ve found that there are very few good introductory materials on the subject and that many new learners get quickly discouraged by the steep learning curve of the subject.</p>
</div>
<div class="section" id="what-is-a-gaussian-process">
<h2>What Is A Gaussian Process?<a class="headerlink" href="#what-is-a-gaussian-process" title="Permalink to this headline">¶</a></h2>
<p>Before we get into Gaussian process models, we should first address what a Gaussian process is.</p>
<p>A <a class="reference external" href="https://www.itl.nist.gov/div898/handbook/pmd/section2/pmd211.htm">process</a> is simply a phenomena that occurs in the world that one might desire to model. For example, if I were to start flipping coins and recording the results I would consider the flipping of the coins to be a process. This is sometimes referred to as the “data generating process”.</p>
<p>This process can be exchangeable or non-exchangeable, meaning that the ordering of samples taken from the process may or may not matter. For example, if I flip a coin many times the flips are almost certainly independent random variables and so the order of the observations does not matter. This means that the data is exchangeable. However, a time series problems such as modeling cards drawn from a deck without replacement requires that I keep track of the order of the observations because they are dependent on each other (once I have drawn the king of hearts I will never draw it again). This kind of data is non-exchangeable.</p>
<p>A process is a <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process#Gaussian_process_prediction,_or_kriging">Gaussian process</a> if any collection of finite random variables drawn from the process is Gaussian distributed. A single sampled random variable would be a univariate Gaussian while a collection of sampled random variables would be a multivariate Gaussian.</p>
<p>Our coin flipping example is obviously not a Gaussian process. Any individual coin flip is <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distributed</a> and any collection of coin flips is <a class="reference external" href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial distributed</a>. We would call the coin flips a <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli process</a>.</p>
<p>Modeling the relationship of height by age in a population that is homogeneous with respect to gender and ethnicity tends to be a Gaussian process. This is because <a class="reference external" href="http://faculty.virginia.edu/ASTR3130/lectures/error2/error2.html">heights at any given age tend to be Gaussian distributed</a> and so modeling heights across ages tends to be a Gaussian process.</p>
<p>There are other common statistical processes of sue to a machine learning engineer other than the Bernoulli and Gaussian processes.</p>
<p>For example, if we were trying to model the distribution of heights by age in a heterogeneous population where the mixture of subpopulations was unobserved we might choose to use a Student-t process. The <a class="reference external" href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t distribution</a> can be thought of as a more general case of the Gaussian distribution (or even as a <a class="reference external" href="http://www.sumsar.net/blog/2013/12/t-as-a-mixture-of-normals/">mixture of Gaussian distributions</a>).</p>
<p>Another popular process is the <a class="reference external" href="https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459">Poisson process</a>, commonly used to model events over time such as mechanical failures or injuries at job sites.</p>
<p>Finally, there is the <a class="reference external" href="https://en.wikipedia.org/wiki/Dirichlet_process">Dirichlet process</a>, commonly used in topic modeling to represent uncertainty over distributions.</p>
<p>Each of these processes could be a chapter (or even a book) in and of themselves. They are each unique in their own way and are used in very different ways. We focus here on Gaussian processes and provide links for learning more about the other processes mentioned above.</p>
</div>
<div class="section" id="what-is-a-gaussian-process-model">
<h2>What is a Gaussian Process Model?<a class="headerlink" href="#what-is-a-gaussian-process-model" title="Permalink to this headline">¶</a></h2>
<p>First of all, I will mention that the term “Gaussian process” and “Gaussian process model” are often used interchangeably. A Gaussian process model is a model where the posterior belief about the data generating process is a Gaussian process. This means that every point prediction that the model makes (the marginal posterior at that point) is a one dimensional Gaussian. Additionally, any collection of predictions can be represented by a multivariate Gaussian distribution.</p>
<p>While the marginal posterior at any point in feature space is a one dimensional Gaussian distribution, the entire posterior is a Gaussian process. This means that the posterior describes a distribution over functions.</p>
<p>This may seem confusing to some, but is actually a very common concept in Bayesian statistics. When fitting a <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_statistics">Bayesian</a> <a class="reference external" href="https://en.wikipedia.org/wiki/General_linear_model">general linear model (GLM)</a> the modeler gets a posterior distribution over model parameters after inference. Sampling values for model parameters from this posterior is equivalent to sampling a function (in the case of a GLM these functions are all linear).</p>
<p>For a Gaussian process we get a posterior over functions without ever having to worry about model parameters! Instead of worrying about model parameters we simply get functions. Wait…how do we get functions without parameters?</p>
<p>We do this using <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Lazy_learning">lazy learning</a>.</p>
<p>This next section is where most students get lost, so I will try describing the concepts below in two different ways: GPs as smoothing models and GPs as linear regression.</p>
<p><em>Attempt One: GPs as Smoothing Models</em></p>
<p>Rather than trying to describe the data using some <a class="reference external" href="https://en.wikipedia.org/wiki/Parametric_model">parametric model</a>, we use kernels to find the relationship (covariance) between labeled points in a training set. When we want to make predictions we use kernels to find the covariance of the new points with our training points. The labels of the new points are arrived at by smoothing (getting a weighted average) over the training points closest to them. How close two points are depends on the kernel being used.</p>
<p><em>Attempt Two: GPs as Linear Regression</em></p>
<p>A different way to think about GPs is to imagine that you are trying to model non-linear data with a linear model. To model the data well you need to transform the data so that it becomes linear. This transformation can be done using kernels. We typically say that the untransformed data is in “feature space” and the transformed data is in “kernel space”. If we pick the right kernel then we will model the data well. If fit this linear model using Bayesian statistics we would get a posterior over the model weights (which is equivalent to getting a posterior over functions where each function is described by a specific value of the model parameters). This is the basic idea behind a GP model <em>except</em> that we never see the model weights. We directly sample outcomes from the posterior rather than sampling parameters and them predicting outcomes.</p>
<p><strong>Author’s Note:</strong> Readers familiar with neural networks might be interested to know that neural networks and Gaussian process models have an interesting relationship. Predictions from a Bayesian neural network can be though of as the sum of random variables where the random variables are the weights of the model. Bayesian statisticians treat these weights as random variables and use inference to find the posterior distribution over these weights. if we imagine that the number of weights go to infinity, then predictions from the model become Gaussian (<a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>). This means that infinitely large Bayesian neural networks are Gaussian processes! It implies that finite sized Bayesian neural networks can be thought of as approximations of true Gaussian process models. In fact, a number of breakthroughs in Bayesian deep learning have come from this line of thinking.</p>
</div>
<div class="section" id="what-is-the-deal-with-kernels">
<h2>What Is The Deal With Kernels?<a class="headerlink" href="#what-is-the-deal-with-kernels" title="Permalink to this headline">¶</a></h2>
<p>In the last section many of you probably picked up on my comment “if we pick the right kernel”. How do we pick the right kernel? What even is a kernel?</p>
<p>A kernel is simply a function that relates two points in space. For example:</p>
<p>$$
k(x_i, x_j) = |x_i - x_j|
$$</p>
<p>This is a very simple kernel. It tells us the absolute different between two points in space. We can think of a kernel as giving us the variance of one point with another. If we apply a kernel over a set of points we get the covariance of all of the points.</p>
<p>$$
K(x, x) =
\begin{bmatrix}
k(x_i, x_j) &amp; \dots  \
\dots &amp; \dots  \
\dots &amp; k(x_n, x_m)
\end{bmatrix}
= cov(x, x)
$$</p>
<p>This brings us to an important constraint on kernels. Kernels must produce <a class="reference external" href="https://math.stackexchange.com/questions/1733726/positive-semi-definite-vs-positive-definite">positive semi-definite</a> matrices. This means that for the matrix produced all values along the diagonal have values &gt;= 0. We put this constraint in place to ensure that the kernel will only produce valid covariance matrices.</p>
<p>Kernels must also be symmetric, meaning that the order of the points being compared should not matter:</p>
<p>$$
k(x_i, x_j) = k(x_j, x_i)
$$</p>
<p><strong>Author’s Note:</strong> In the last section I noted that neural networks and Gaussian processes share a philosophical relationship based on Bayesian neural network behavior in the limit of infinitely many parameters. neural networks also relate to GPs where kernels are concerned. Neural networks can be considered <a class="reference external" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators </a>. This means that (theoretically) they can approximate arbitrarily complex functions as their number of parameters increases. We can take advantage of this for GPs by using a neural network as a kernel function. Rather than hand picking a kernel function we let the neural network learn a kernel that works for us. We call such a kernel a “general kernel”.</p>
</div>
<div class="section" id="advantages-of-gaussian-process-models">
<h2>Advantages Of Gaussian Process Models<a class="headerlink" href="#advantages-of-gaussian-process-models" title="Permalink to this headline">¶</a></h2>
<p>So what is so great about these models? Firstly, they fit nicely into the Bayesian modeling paradigm. We set priors, collect data, and use inference to get a posterior. This provides us with a model of the data generating process we are interested with predictions and uncertainty baked in.</p>
<p>Additionally, GPs can model highly non-linear data, unlike many Bayesian models. If we have very complex data we can feel more confident about getting a good fit.</p>
</div>
<div class="section" id="disadvantages-of-gaussian-process-models">
<h2>Disadvantages Of Gaussian Process Models<a class="headerlink" href="#disadvantages-of-gaussian-process-models" title="Permalink to this headline">¶</a></h2>
<p>GPs can be hard to fit. Choosing the right kernel can be difficult and might require a fair bit of domain specific knowledge.</p>
<p>Additionally, the lazy learning characteristics of a GP model mean that fitting and prediction can be computationally taxing.</p>
<p>There is a lot of research out there on how to efficiently fit and/or approximate a Gaussian process model to mitigate these problems.</p>
</div>
<div class="section" id="what-have-we-learned">
<h2>What Have We Learned?<a class="headerlink" href="#what-have-we-learned" title="Permalink to this headline">¶</a></h2>
<p>We learned that a Gaussian process is simply a set of events that are produced in such a way that any (or every) collection of finite random variables from the process is Gaussian distributed.</p>
<p>We learned that a Gaussian process model is a Bayesian modeling method that uses kernels to produce non-linear predictive models with a Gaussian process posterior.</p>
<p>We learned that Gaussian process models are most useful for modeling highly non-linear data where it is important to get uncertainty estimates around out model predictions.</p>
</div>
<div class="section" id="where-to-learn-more">
<h2>Where To Learn More<a class="headerlink" href="#where-to-learn-more" title="Permalink to this headline">¶</a></h2>
<p>We’ve barely scratched the surface on Gaussian process models. Below I list other useful educational resources that you might want to explore. I don’t agree with 100% of the methods and advice for fitting Gaussian process models in the materials below, but they are all good introductions to GP modeling for the beginner.</p>
<p>Fun Visualizations:</p>
<ul class="simple">
<li><a class="reference external" href="https://chi-feng.github.io/gp-demo/">GP Demo</a></li>
<li><a class="reference external" href="http://www.tmpl.fi/gp/">Gaussian Process Regression Demo</a></li>
</ul>
<p>More About Kernels:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.cs.toronto.edu/%7Eduvenaud/cookbook/index.html">The Kernel Cookbook</a></li>
<li><a class="reference external" href="http://www.cs.toronto.edu/%7Eduvenaud/thesis.pdf">David Duvenaud’s Thesis</a></li>
</ul>
<p>Introductory Level Material:</p>
<ul class="simple">
<li><a class="reference external" href="https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d">An intuitive guide to Gaussian processes</a></li>
<li><a class="reference external" href="http://katbailey.github.io/post/gaussian-processes-for-dummies/">Gaussian Processes for Dummies</a></li>
<li><a class="reference external" href="http://www.cs.cmu.edu/%7E16831-f14/notes/F09/lec20/16831_lecture20.albertor.pdf">Statistical Techniques in Robotics: Gaussian Process Part One</a></li>
<li><a class="reference external" href="http://www.cs.cmu.edu/%7E16831-f14/notes/F09/lec21/16831_lecture21.sross.pdf">Statistical Techniques in Robotics: Gaussian Process Part Two</a></li>
</ul>
<p>Practitioner Level Material:</p>
<ul class="simple">
<li><a class="reference external" href="https://www.robots.ox.ac.uk/%7Emebden/reports/GPtutorial.pdf">Gaussian Processes for Regression:
A Quick Introduction</a></li>
<li><a class="reference external" href="http://mlg.eng.cam.ac.uk/tutorials/06/es.pdf">Tutorial: Gaussian process models
for machine learning</a></li>
<li><a class="reference external" href="https://www.cs.cmu.edu/%7Eepxing/Class/10708-17/notes-17/10708-scribe-lecture24.pdf"> Gaussian Process and Deep Kernel Learning</a></li>
<li><a class="reference external" href="http://people.ee.duke.edu/%7Elcarin/David1.27.06.pdf">Gaussian Processes</a></li>
</ul>
<p>The GP Bible:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">Gaussian Processes for Machine Learning</a></li>
</ul>
<p>Notes and Guides:</p>
<ul class="simple">
<li><a class="reference external" href="http://keeganhin.es/blog/gp.html">Notes On Gaussian Processes</a></li>
<li><a class="reference external" href="https://drafts.distill.pub/gp/">A Practical Guide to Gaussian Processes</a></li>
<li><a class="reference external" href="https://www.jgoertler.com/visual-exploration-gaussian-processes/">A Visual Exploration of Gaussian Processes</a></li>
</ul>
<p>Packages For GP Modeling:</p>
<ul class="simple">
<li><a class="reference external" href="https://gpytorch.ai/">GPyTorch</a></li>
<li><a class="reference external" href="http://docs.pyro.ai/en/0.3.1/contrib.gp.html">Pyro</a></li>
<li><a class="reference external" href="https://gpy.readthedocs.io/en/deploy/">GPy</a></li>
<li><a class="reference external" href="https://gpflow.readthedocs.io/en/develop/">GPFlow</a></li>
<li><a class="reference external" href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Process_Regression_In_TFP.ipynb">Tensorflow Probability</a></li>
<li><a class="reference external" href="https://scikit-learn.org/stable/modules/gaussian_process.html">Sklearn</a></li>
<li><a class="reference external" href="https://george.readthedocs.io/en/latest/tutorials/first/">George</a></li>
<li><a class="reference external" href="https://docs.pymc.io/api/gp.html">Pymc3</a></li>
<li><a class="reference external" href="https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html">Stan</a></li>
<li><a class="reference external" href="https://github.com/James-Montgomery/squidward">Squidward</a></li>
</ul>
<p>Information about Other Processes</p>
<ul class="simple">
<li><a class="reference external" href="https://www.ee.ryerson.ca/%7Ecourses/ee8103/chap4.pdf">Stochastic Processes</a></li>
<li><a class="reference external" href="https://www.unf.edu/%7Ecwinton/html/cop4300/s09/class.notes/DiscreteDist.pdf">Bernoulli and Poisson</a></li>
<li><a class="reference external" href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec20.pdf">Bernoulli and Poisson</a></li>
<li><a class="reference external" href="http://astrostatistics.psu.edu/su05/richards_poisson061005.pdf">Poisson Processes and Gaussian Processes</a></li>
<li><a class="reference external" href="https://fromosia.wordpress.com/2017/03/19/stochastic-poisson-process/">Poisson</a></li>
<li><a class="reference external" href="https://www.probabilitycourse.com/chapter11/11_1_2_basic_concepts_of_the_poisson_process.php">Basic Concepts of the Poisson Process</a></li>
<li><a class="reference external" href="https://www.cs.cmu.edu/%7Ekbe/dp_tutorial.pdf">Dirichlet Processes: A Gentle Tutorial</a></li>
<li><a class="reference external" href="https://www.ijcai.org/proceedings/2017/0393.pdf">Student-t Process Regression with Student-t Likelihood</a></li>
<li><a class="reference external" href="https://www.cs.cmu.edu/%7Eandrewgw/tprocess.pdf">Student-t Processes as Alternatives to Gaussian Processes</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Fitting_A_GP.html" class="btn btn-neutral float-right" title="Fitting A Gaussian Process" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Classification.html" class="btn btn-neutral float-left" title="Gaussian Process Classification" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, James Montgomery

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>